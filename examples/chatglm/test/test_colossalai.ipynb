{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '0525coloss' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n 0525coloss ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from colossalai.nn import LayerNorm1D\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        hidden_size=4096\n",
    "        layernorm_epsilon=1e-5\n",
    "        dtype=torch.float16\n",
    "        self.input_layernorm = LayerNorm1D(hidden_size, eps=layernorm_epsilon, dtype=dtype)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self,inputs):\n",
    "        res=self.input_layernorm(inputs)\n",
    "        print(self.input_layernorm)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 4096])\n",
      "tensor(1.8887, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=torch.randn(4,1,4096)\n",
    "data = data.clone().detach().to(device='cuda:0', dtype=torch.float16)\n",
    "# data = data.clone().detach().to(device='cuda:0', dtype=torch.float16)\n",
    "# data=torch.tensor(data=data,device='cuda:0',dtype=torch.float16)\n",
    "print(data.shape)\n",
    "print(data[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/colossalai/kernel/op_builder/builder.py:161\u001b[0m, in \u001b[0;36mBuilder.load\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[39m# if the kernel has been pre-built during installation\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# we just directly import it\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     op_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimport_op()\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/colossalai/kernel/op_builder/builder.py:110\u001b[0m, in \u001b[0;36mBuilder.import_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mThis function will import the op module by its string name.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprebuilt_import_path)\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'colossalai._C.layernorm'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m diymodel\u001b[39m=\u001b[39mModel()\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      2\u001b[0m diymodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 3\u001b[0m res\u001b[39m=\u001b[39mdiymodel(data)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(res\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(res[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/nn/modules/module.py:1193\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks  \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1194\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,inputs):\n\u001b[0;32m---> 11\u001b[0m     res\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(inputs)\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/nn/modules/module.py:1193\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks  \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1194\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/colossalai/nn/layer/colossalai_layer/_utils.py:41\u001b[0m, in \u001b[0;36mColossalaiModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/nn/modules/module.py:1193\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks  \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1194\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/colossalai/kernel/cuda_native/layer_norm.py:75\u001b[0m, in \u001b[0;36mMixedFusedLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m FusedLayerNormAffineFunction\u001b[39m.\u001b[39;49mapply(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py:105\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[39mreturn\u001b[39;00m fwd(\u001b[39m*\u001b[39m_cast(args, cast_inputs), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_cast(kwargs, cast_inputs))\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m fwd(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/colossalai/kernel/cuda_native/layer_norm.py:34\u001b[0m, in \u001b[0;36mFusedLayerNormAffineFunction.forward\u001b[0;34m(ctx, input, weight, bias, normalized_shape, eps)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mglobal\u001b[39;00m layer_norm\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m layer_norm \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     layer_norm \u001b[39m=\u001b[39m LayerNormBuilder()\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     35\u001b[0m output, mean, invvar \u001b[39m=\u001b[39m layer_norm\u001b[39m.\u001b[39mforward_affine(input_, ctx\u001b[39m.\u001b[39mnormalized_shape, weight_, bias_, ctx\u001b[39m.\u001b[39meps)\n\u001b[1;32m     36\u001b[0m ctx\u001b[39m.\u001b[39mlayernorm_op \u001b[39m=\u001b[39m layer_norm\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/colossalai/kernel/op_builder/builder.py:187\u001b[0m, in \u001b[0;36mBuilder.load\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    184\u001b[0m     print_rank_0(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[extension] Compiling or loading the JIT-built \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m kernel during runtime now\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[39m# load the kernel\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m op_module \u001b[39m=\u001b[39m load(name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    188\u001b[0m                  sources\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msources_files()),\n\u001b[1;32m    189\u001b[0m                  extra_include_paths\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minclude_dirs()),\n\u001b[1;32m    190\u001b[0m                  extra_cflags\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcxx_flags(),\n\u001b[1;32m    191\u001b[0m                  extra_cuda_cflags\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnvcc_flags(),\n\u001b[1;32m    192\u001b[0m                  extra_ldflags\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    193\u001b[0m                  build_directory\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m    194\u001b[0m                  verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    196\u001b[0m build_duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_build\n\u001b[1;32m    198\u001b[0m \u001b[39m# log jit compilation time\u001b[39;00m\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1284\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name,\n\u001b[1;32m   1193\u001b[0m          sources: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m   1194\u001b[0m          extra_cflags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m          is_standalone\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1203\u001b[0m          keep_intermediates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[39m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39m        ...     verbose=True)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m-> 1284\u001b[0m     \u001b[39mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1285\u001b[0m         name,\n\u001b[1;32m   1286\u001b[0m         [sources] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(sources, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m sources,\n\u001b[1;32m   1287\u001b[0m         extra_cflags,\n\u001b[1;32m   1288\u001b[0m         extra_cuda_cflags,\n\u001b[1;32m   1289\u001b[0m         extra_ldflags,\n\u001b[1;32m   1290\u001b[0m         extra_include_paths,\n\u001b[1;32m   1291\u001b[0m         build_directory \u001b[39mor\u001b[39;49;00m _get_build_directory(name, verbose),\n\u001b[1;32m   1292\u001b[0m         verbose,\n\u001b[1;32m   1293\u001b[0m         with_cuda,\n\u001b[1;32m   1294\u001b[0m         is_python_module,\n\u001b[1;32m   1295\u001b[0m         is_standalone,\n\u001b[1;32m   1296\u001b[0m         keep_intermediates\u001b[39m=\u001b[39;49mkeep_intermediates)\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1522\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             baton\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1521\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1522\u001b[0m         baton\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   1523\u001b[0m \u001b[39melif\u001b[39;00m verbose:\n\u001b[1;32m   1524\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNo modifications detected for re-loaded extension \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1525\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, skipping build step...\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1526\u001b[0m           file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n",
      "File \u001b[0;32m/data2/zxy/miniconda3/envs/n_energon/lib/python3.8/site-packages/torch/utils/file_baton.py:42\u001b[0m, in \u001b[0;36mFileBaton.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mPeriodically sleeps for a certain amount until the baton is released.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39mThe amount of time slept depends on the ``wait_seconds`` parameter\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mpassed to the constructor.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mwhile\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlock_file_path):\n\u001b[0;32m---> 42\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diymodel=Model().cuda()\n",
    "diymodel.eval()\n",
    "res=diymodel(data)\n",
    "\n",
    "print(res.shape)\n",
    "print(res[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n_energon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
